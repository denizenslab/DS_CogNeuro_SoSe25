{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression\n",
    "In the last lab and in the homework, we saw how we can model the response of a voxel $v$ to a stimulus $i$ via the hemodynamic response function, and a weight $\\beta^v_i$. To do this we assume that we know the shape of the hemodynamic response, and we assume a value for how well a voxel responds to a stimulus. But in reality, we do not know these. The hemodynamic response has been a subject for a lot of research and therefore it is common to use a canonical function like the one we explored last week. However, estimating how much a voxel responds to a specific stimulus is exactly what the purpose of an fMRI experiment is.\n",
    "\n",
    "# Goals\n",
    "\n",
    "- The regression problem and it's solution\n",
    "- Linear regression with real data\n",
    "- Multiple regression\n",
    "- Contrast maps in the assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import neurods as nds\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# Configure defaults for plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.aspect'] = 'auto'\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's talk about linear regression, and start to work with a simple problem. Let's say you walk into a grocery store, and you buy 3 oranges and 2 apples. You are told the price is 8$\\$$. You go another time, you buy 2 oranges and 5 apples, you pay 9$\\$$. How much do apples and oranges cost?\n",
    "\n",
    "This is a system of two equations with two unknown that you can solve and obtain an exact solution.\n",
    "\n",
    "However, imagine that the cashier doesn't tell you the exact price, but takes the correct price, and then depending on their mood, adds some \"noise\" to the price: you either pay a little less or a little more. This noise doesn't depend on how much your total is, but on other unrelated factors.\n",
    "\n",
    "Can you still estimate accurately the prices if you go twice to the store?\n",
    "\n",
    "What if you go 1000 times?\n",
    "\n",
    "Let's simulate this with a random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# these are the hidden parameters, we are not supposed to know them:\n",
    "apple_price = 0.9\n",
    "orange_price = 1.2\n",
    "pear_price = 1.5\n",
    "noise_variance = 5\n",
    "\n",
    "# sample X and Y:\n",
    "n = 1000\n",
    "X = np.round(np.random.uniform(low = 0, high = 10, size=[n,3])).astype(int)\n",
    "real_beta = np.array([apple_price, orange_price, pear_price]).reshape([3,1])\n",
    "Y = np.dot(X, real_beta) + np.random.normal(size =[n,1] )*noise_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can express the price in one visit j as:\n",
    "\n",
    "\\begin{align}\n",
    "y_j =  X_j \\beta +\\epsilon_j\n",
    "\\end{align}\n",
    "\n",
    "Were the row $X_j = [x^a_j,x^o_j,x^p_j]$ corresponds to the counts $x^a_j,x^o_j,x^p_j$ of apples, oranges and pears bought on visit $j$, and $\\beta = [\\beta_a,\\beta_o,\\beta_p]$ corresponds to the prices  $\\beta_a,\\beta_o,\\beta_p$ of apples, oranges and pears. \n",
    "\n",
    "We can write the entire visits as:\n",
    "\n",
    "\\begin{align}\n",
    "Y =  {\\bf X} \\beta +\\epsilon\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "- $Y$ is n x 1\n",
    "- ${\\bf X}$ is n x d, here d = 3\n",
    "- ${\\beta}$ is d x 1\n",
    "- $\\epsilon$ is n x 1.\n",
    "\n",
    "Due to the noise, we cannot exactly recover $\\beta$. However, we would like to find a solution $\\hat\\beta$ that minimizes the following error as much as possible:\n",
    "\n",
    "\\begin{align}\n",
    "error = \\sum_{j = 1}^N (y_j - X_j \\beta)^2 = ||Y - {\\bf X} \\beta||_2^2\n",
    "\\end{align}\n",
    "\n",
    "This is the sum of squared errors. To minimize this equation with respect to $\\beta$, we first find the derivative with respect to $\\beta$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta \\ error}{\\delta \\beta} &=& \\frac{\\delta ||Y - {\\bf X} \\beta||_2^2}{\\delta \\beta}\\\\\n",
    " &=& -2{\\bf X}^\\top (Y - {\\bf X} \\beta)\\\\\n",
    "\\end{align}\n",
    "\n",
    "The minimum is achieved when the derivative is zero:\n",
    "\n",
    "\\begin{align}\n",
    "-2{\\bf X}^\\top (Y - {\\bf X} \\hat\\beta) = 0\\\\\n",
    "{\\bf X}^\\top Y = {\\bf X}^\\top{\\bf X} \\hat\\beta\\\\\n",
    "\\hat\\beta = ({\\bf X}^\\top{\\bf X})^{-1}{\\bf X}^\\top Y\\\\\n",
    "\\end{align}\n",
    "\n",
    "This is the Ordinary Least Squares Solution. Now write it as a function, then use this function to recover the prices of the fruits, using the following cell:\n",
    "\n",
    "### BREAKOUT SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "def OLS(X,Y):\n",
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prices = OLS(X,Y)\n",
    "print(\"apple price: real {0} estimated {1}\".format(apple_price,prices[0,0]))\n",
    "print(\"orange price: real {0} estimated {1}\".format(orange_price,prices[1,0]))\n",
    "print(\"pear price: real {0} estimated {1}\".format(pear_price,prices[2,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to change the number of datapoints and the magnitude of the noise, what do you notice?\n",
    "\n",
    "## Intercept Term\n",
    "\n",
    "Many times, we are interested in modeling $y_j$ as:\n",
    "\n",
    "\\begin{align}\n",
    "y_j =  \\beta_0 + \\beta_1 x^1_j + \\beta_2 x^2_j ... + \\beta_3 x^d_j +\\epsilon_j\n",
    "\\end{align}\n",
    "\n",
    "this means there is a constant intercept term which is always contributed to the output $y_j$. In our store analogy, this could be for example an additional flat fare that is added by the cashier for each costumer. How can we integrate the intercept term in our framework?\n",
    "\n",
    "There is a simple way, notice how we can rewrite the above equation as:\n",
    "\n",
    "\\begin{align}\n",
    "y_j =  \\beta_0 x^0_j+ \\beta_1 x^1_j + \\beta_2 x^2_j ... + \\beta_3 x^d_j +\\epsilon_j\n",
    "\\end{align}\n",
    "\n",
    "where $x^0_j$ is always equal to 1. This can be done by creating a matrix $X'$ by adding an additional column to our matrix $X$ which is all ones. Let's try to estimate the intercept term in our fruit example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X2 = np.hstack([np.ones([n,1]),X])\n",
    "prices = OLS(X2,Y)\n",
    "print('intercept term is estimated to be {0}'.format(prices[0,0]))\n",
    "print(\"apple price: real {0} estimated {1}\".format(apple_price,prices[1,0]))\n",
    "print(\"orange price: real {0} estimated {1}\".format(orange_price,prices[2,0]))\n",
    "print(\"pear price: real {0} estimated {1}\".format(pear_price,prices[3,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept term is estimated to be 0, which makes sense because we didn't specify an intercept term! Let's sample data another time and estimate the intercept again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "intercept_term = 2\n",
    "Y2 = np.dot(X, real_beta) + np.random.normal(size =[n,1] )*noise_variance + intercept_term\n",
    "\n",
    "prices = OLS(X2,Y2)\n",
    "print('intercept term is estimated to be {0}'.format(prices[0,0]))\n",
    "print(\"apple price: real {0} estimated {1}\".format(apple_price,prices[1,0]))\n",
    "print(\"orange price: real {0} estimated {1}\".format(orange_price,prices[2,0]))\n",
    "print(\"pear price: real {0} estimated {1}\".format(pear_price,prices[3,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we work with fMRI data, we usually remove the mean of each voxel in the begining of our analysis. This means we don't need to include the intercept term in our design matrix, because it's effectively equal to zero.\n",
    "\n",
    "## Modeling voxel responses\n",
    "\n",
    "\n",
    "\n",
    "Remember, we are using regression because we want to model different voxel responses to a set of stimuli. We learned how to take a stimulus time course and how to convolve it with the hemodynamic response. We then assumed that each voxel's activity was a linear combination of all the convolved time courses of the stimuli. We want to recover the parameters of the linear combination. Let's load some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "basedir = '../../data/fMRI/categories/'\n",
    "design = np.load(os.path.join(basedir,'experiment_design.npz'))\n",
    "print('Experiment design variables: ', design.keys())\n",
    "conditions = design['conditions'].tolist()\n",
    "print('Conditions: ', conditions)\n",
    "design_run1 = design['run1']\n",
    "plt.figure()\n",
    "for i, (cond, label) in enumerate(zip(design_run1.T, conditions)):\n",
    "    plt.plot(cond+i+0.2*i, label=label, lw=2)\n",
    "plt.title('Condition labels')\n",
    "_ = plt.legend(frameon=False, bbox_to_anchor=(1.4, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the zscore function while loading the data to normalize every block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def load_data(fname, mask=None, standardize=False):\n",
    "    \"\"\"Load fMRI data from nifti file, optionally with masking and standardization\"\"\"\n",
    "    if isinstance(fname, (list, tuple)):\n",
    "        return np.vstack([load_data(f, mask=mask, standardize=standardize) for f in fname])\n",
    "    nii = nibabel.load(fname)\n",
    "    data = nii.get_fdata().T.astype(np.float32)\n",
    "    if mask is not None:\n",
    "        data = data[:, mask]\n",
    "    if standardize:\n",
    "        data = scipy.stats.zscore(data, axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import cortex\n",
    "\n",
    "sub, xfm = 's01', 'catloc'\n",
    "\n",
    "mask = cortex.db.get_mask(sub, xfm, type='cortical')\n",
    "fname = os.path.join(basedir, 's01_categories_{n}.nii.gz') \n",
    "# fmri responses in all runs:\n",
    "Y = np.vstack([zscore(load_data(fname.format(n=n), mask=mask, standardize=True)) for n in ['01', '02', '03']])\n",
    "# stimuli:\n",
    "X = np.vstack([design[run] for run in ['run1','run2', 'run3']])\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.imshow(Y.T)\n",
    "plt.title('Voxel responses')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "for i, (cond, label) in enumerate(zip(X.T, conditions)):\n",
    "    plt.plot(cond+i+0.2*i, label=label, lw=2)\n",
    "plt.title('Condition labels')\n",
    "_ = plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first build a design matrix that accounts for the hemodynamic response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from neurods.fmri import hrf as generate_hrf\n",
    "t_hrf, hrf_1 = generate_hrf(tr=2)\n",
    "n, d = X.shape\n",
    "\n",
    "conv_X = np.zeros_like(X, dtype=float)\n",
    "for i in range(d):\n",
    "    conv_X[:,i] = np.convolve(X[:,i], hrf_1)[:n]\n",
    "\n",
    "plt.figure()\n",
    "for i, (cond, label) in enumerate(zip(conv_X.T, conditions)):\n",
    "    plt.plot(cond+i+0.2*i, label=label, lw=2)\n",
    "    \n",
    "plt.title('Condition labels')   \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression for real fMRI data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the response of all the voxels in the brain to these 5 different conditions. Instead of a one dimensional output $Y$, we have a high dimensional output ${\\bf Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to learn how each voxel responds to the stimulus. For each voxel, we can write the linear model equation as:\n",
    "\n",
    "\\begin{align}\n",
    "y^{v}_j =  X_j \\beta^v +\\epsilon_j^v\n",
    "\\end{align}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align}\n",
    "Y^{v} =  {\\bf X} \\beta^v +\\epsilon^v\n",
    "\\end{align}\n",
    "\n",
    "Since this model exist for every function, we can write it as a multiple regression function:\n",
    "\\begin{align}\n",
    "{\\bf Y} =  {\\bf X} \\boldsymbol\\beta +\\boldsymbol\\epsilon\n",
    "\\end{align}\n",
    "\n",
    "In the above:\n",
    "- ${\\bf Y}$ is n x nVoxels\n",
    "- ${\\bf X}$ is n x d\n",
    "- ${\\boldsymbol \\beta}$ is d x nVoxels\n",
    "\n",
    "Let's try to minimize the **sum of squared errors (SSE)** like before with respect to $\\boldsymbol\\beta$, we first find the derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta \\ error}{\\delta \\boldsymbol \\beta} &=& \\frac{\\delta ||{\\bf Y} - {\\bf X} \\boldsymbol \\beta||_2^2}{\\delta  \\boldsymbol \\beta}\\\\\n",
    " &=& -2{\\bf X}^\\top ({\\bf Y} - {\\bf X} \\boldsymbol \\beta)\\\\\n",
    "\\end{align}\n",
    "\n",
    "The minimum is achieved when the derivative is zero:\n",
    "\n",
    "\\begin{align}\n",
    "-2{\\bf X}^\\top ( {\\bf Y} - {\\bf X} \\boldsymbol\\beta) = 0\\\\\n",
    "{\\bf X}^\\top {\\bf Y} = {\\bf X}^\\top{\\bf X}  \\boldsymbol\\beta\\\\\n",
    "\\boldsymbol \\beta = ({\\bf X}^\\top{\\bf X})^{-1}{\\bf X}^\\top {\\bf Y}\\\\\n",
    "\\end{align}\n",
    "\n",
    "This solution is similar to the single dimensional output solution. The first term $({\\bf X}^\\top{\\bf X})^{-1}{\\bf X}^\\top$ is independent of the data. If we are estimating the parameters for one voxel, or for a large number, this term will be the same. \n",
    "\n",
    "Notice also that each voxel's parameters are estimated independently from each other: each column of $\\boldsymbol \\beta$ corresponds to the parameters of one voxel $v$, and it is obtained by multipling the matrix $({\\bf X}^\\top{\\bf X})^{-1}{\\bf X}^\\top$ with the ${\\bf Y}$ column that corresponds to voxel $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's continue applying linear models to real fMRI data. As introduced in last lecture, this time we'll use a linear regression estimator from scikit-learn to get another glimpse at this software package. We'll fit the faces response vector to our FFA voxel and see how we do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "conv_X_faces = conv_X[:, 1]\n",
    "# get the faces voxels\n",
    "Y_faces = Y[:, 3464]\n",
    "\n",
    "reg_model_faces = LinearRegression()\n",
    "reg_model_faces.fit(conv_X_faces, Y_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-Oh, why didn't that work? Remember that we need to reshape vectors to 1-D matrices. This is annoying now, but will become useful once we want to find regression models to multiple voxels at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_faces = LinearRegression(fit_intercept=True)\n",
    "reg_model_faces.fit(conv_X_faces.reshape(-1,1), Y_faces.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, that did work now!\n",
    "\n",
    "OK. Where is the info hidden? In all scikit-learn estimators, the properties computed during `fit` are stored in names with *trailing underscore*. In this case it is `.intercept_` and `.coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_faces.intercept_, reg_model_faces.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, these values correspond to `slope` and `intercept` computed above. \n",
    "\n",
    "Let's see how good this model is by calculating the sum of squared errors (SSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_faces_pred_faces = reg_model_faces.predict(conv_X_faces.reshape(-1,1))[:,0]\n",
    "\n",
    "faces_sse = np.sum((Y_faces - data_faces_pred_faces) **2)\n",
    "faces_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data and the regression line to see how well it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(conv_X_faces, data_faces_pred_faces, 'k', label='Regression Line')\n",
    "plt.plot(conv_X_faces, Y_faces, '.r', label='Face Data')\n",
    "plt.xlabel('Faces Response')\n",
    "plt.ylabel('BOLD Signal')\n",
    "plt.title('FFA Voxel Regression')\n",
    "_ = plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Fit a linear regression model to the PPA voxel BOLD data using the `places` response vector. Plot the data as a scatter plot, and the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_X_places = \n",
    "# get the faces voxels\n",
    "Y_places = Y[:, 10433]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "The real power of regression comes when we have more than one **independent variable** we want to associate with the **dependent variable**.\n",
    "\n",
    "The case of multiple linear regression is a natural extension of the simple linear regression case. In multiple regression, we have multiple independent variables ($x^i$), and still a single dependent variable (y). Now the equation describing each time point of data looks like this: \n",
    "\n",
    "\\begin{align}\n",
    "y_j =  w_0 + w_1 x^1_j + w_2 x^2_j + \\dots + w_d x^d_j +\\varepsilon_j\n",
    "\\end{align}\n",
    "\n",
    "We can rewrite this second equation for multiple regression using a more succinct notation:\n",
    "\n",
    "\\begin{align}\n",
    "Y =  {\\bf X}W + w_0 +\\varepsilon,\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "- $Y$ is the n x 1 array of BOLD data\n",
    "- ${\\bf X}$ is the n x d matrix were each of the columns is a response vector (${\\bf X}_{ji} = x^i_j$)\n",
    "- ${W}$ is d x 1, contains all the $w_i$\n",
    "- $\\varepsilon$ is the n x 1 vector of errors, or residuals\n",
    "\n",
    "We can write the solution to this multiple regression model as the following equation, which is what we will use to fit our model. This is just for your own information - you don't need to know this equation for any exams:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat W = ({\\bf X}^\\top{\\bf X})^{-1}{\\bf X}^\\top Y\\\\\n",
    "\\end{align}\n",
    "\n",
    "Even though the above formula is not necessary for exams, take a good look at it, and try to peel apart what it means. It is likely that you will encounter it more than once throughout your career. Disregard the $({\\bf X}^\\top{\\bf X})^{-1}$ for a bit and take a look at the rest (${\\bf X}^\\top Y$). Does it ring a bell? If not, refer back to the non-exam-relevant interlude from above.\n",
    "\n",
    "As it turns out, the equation that minimizes the Sum of Squared Errors (SSE) is exactly the same for the multiple regression case as it is for the simple linear regression case, so we can use the same `LinearRegression` function for both! Let's explore this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the Face Model: Modeling Faces and Bodies\n",
    "\n",
    "Since we know that the subject saw more than just images of faces, it makes sense that we would want to model more than just the response to faces in order to get a model that better explains the BOLD data (y). Now let's see what happens if we include the bodies response vector in the model as well..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition using time series plots\n",
    "\n",
    "First let's plot the FFA BOLD data along with the response vectors for the faces and bodies to remind ourselves what the signal looks like when bodies were shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_X_bodies = conv_X[:, 0]\n",
    "_ = plt.figure(figsize=(10, 8))\n",
    "_ = plt.plot(Y_faces, label = 'Bold Data')\n",
    "_ = plt.plot(conv_X_faces, label = 'Faces')\n",
    "_ = plt.plot(conv_X_bodies, label = 'Bodies')\n",
    "_ = plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the **design matrix**, for just bodies and faces. We'll use `np.stack` to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_faces_bodies = np.stack((conv_X_faces, conv_X_bodies), axis=1)\n",
    "print(X_faces_bodies.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(X_faces_bodies, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reshape the FFA voxel BOLD data into a 2-D matrix with one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_faces_2D = Y_faces.reshape(len(Y_faces),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "\n",
    "Now let's fit a multiple regression model using both the faces and the bodies response vectors as independent variables. We just learned that the same equation estimates the coefficients for simple and multiple linear regression, so we can use the same `LinearRegression` object to fit this multiple regression model. Instead of reshaping a single response vector into a 2-D matrix, we'll use the design matrix we just created. We'll use the reshaped FFA voxel BOLD data and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_face_body = LinearRegression()\n",
    "reg_model_face_body.fit(X_faces_bodies, Y_faces_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the coefficient for faces from this model and from the first regression model of faces that we built. There are 2 slope coefficients now, and since the faces response vector was in the first column, the first coefficient is for faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Faces Coefficient with Bodies: %.03f' % (reg_model_face_body.coef_[0][0]))\n",
    "print('Faces Coefficient Only: %.03f' % (reg_model_faces.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two coefficients are similar, but not the same! We'll learn more about why this is later on. \n",
    "\n",
    "For now let's predict the original data using this faces and bodies model, and then calculate the sum of squared error (SSE) of this new model. Then we can compare this SSE value with the SSE we calculated above for the faces only model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faces_pred_facebody = reg_model_face_body.predict(X_faces_bodies)[:, 0]\n",
    "faces_bodies_sse = np.sum((Y_faces - data_faces_pred_facebody) ** 2)\n",
    "faces_bodies_sse, faces_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the SSE for the faces bodies model is lower than that for the faces only model we can conclude that including the bodies response vector into the model improves the fit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Full Model with All the Categories\n",
    "\n",
    "We saw that the model that incorporated bodies and faces did a better job than the model which just incorporated faces. Let's see what happens when we include the response vectors that describe all the stimuli that the subject saw.\n",
    "\n",
    "We'll do this for 5 voxels that are selective to the different stimulus types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_indices = np.array([16521, 3464, 8701, 8318, 8806, 10433])\n",
    "five_voxels = Y[:, voxel_indices]\n",
    "five_voxels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the time series of these 5 voxels to get an idea of what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(voxel_indices)):\n",
    "    timeseries = five_voxels[:, i]\n",
    "    plt.plot(timeseries + 5 * i)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit a multiple regression model in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_full = LinearRegression()\n",
    "reg_model_full.fit(conv_X, five_voxels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's predict and calculate the SSE again for the faces voxel, so we can compare it with the faces bodies model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faces_pred_full = reg_model_full.predict(conv_X)[:,1]\n",
    "full_sse = np.sum((Y_faces-data_faces_pred_full)**2)\n",
    "print('Just using Faces and bodies, SSE is: %.02f, adding all categories SSE is: %.02f' % (faces_bodies_sse, full_sse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the SSE is slightly lower when adding all of the response vectors, but not by much! What matters in statistics is whether this is **significantly** lower. We'll learn about significance testing in the next lecture. \n",
    "\n",
    "Because adding the remaining regressors does not seem to reduce the error very much anymore, it seems unlikely that our voxel responds significantly to the added categories. However, using the full set of response vectors in regression is going to be useful when working with the full brain - there may be voxels that are very well modeled by these additional response vectors.\n",
    "\n",
    "Finally, let's plot the real BOLD data time series along with the predicted time series from using multiple regression to visualize how well the model fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = reg_model_full.predict(conv_X)\n",
    "\n",
    "weights = reg_model_full.coef_\n",
    "intercepts = reg_model_full.intercept_\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(voxel_indices)):\n",
    "    timeseries = five_voxels[:, i]\n",
    "    prediction = predictions[:, i]\n",
    "    plt.plot(timeseries + 5 * i, lw=2)\n",
    "    plt.plot(prediction + 5 * i, lw=2)\n",
    "    \n",
    "    # Now let's display the individual regressors\n",
    "    for j in range(5):\n",
    "        plt.plot(conv_X[:,j] * weights[i, j] + intercepts[i] + 5 * i, lw=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Do We Need Multiple Regression?\n",
    "\n",
    "We just saw that simple and multiple linear regression don't always estimate the same value for coefficients of the same $x$ variable (the faces response). So why would we want to use multiple regression instead of estimating the coeffcicents of a number of simple linear regression models?\n",
    "\n",
    "The answer is that when there is a correlation between the $x$ variables, multiple regression can account for that correlation, whereas doing several simple regressions cannot. In other words, multiple regression controls for correlation in the independent variables. This leads to a more accurate estimation of all the slopes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

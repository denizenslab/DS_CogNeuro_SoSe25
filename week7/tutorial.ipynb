{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to functional magnetic resonance imaging (fMRI): loading and plotting data in Python and PyCortex\t\n",
    "\n",
    "\n",
    "\n",
    "## Introduction to functional magnetic resonance imaging (fMRI)\n",
    "\n",
    "In this session, we will learn about fMRI data properties by loading, manipulating and visualizing it.\n",
    "\n",
    "# Goals for today\n",
    "\n",
    "We will go over some important concepts of data manipulation and visualization in fMRI, including: \n",
    "\n",
    "* Displaying a data (functional volumes) as an image (`plt.imshow()`)\n",
    "* Selecting an appropriate colormap for data visualization\n",
    "* Displaying a 3D array as a mosaic or contact sheet of images\n",
    "* Plotting timecourses of fMRI data\n",
    "* Masking / unmasking of fMRI data\n",
    "\n",
    "By the end, we will have written functions to:\n",
    "\n",
    "* Display a 3D array as a mosaic of images\n",
    "* Unmask an array\n",
    "* Normalize a timeseries\n",
    "\n",
    "## Short overview of fMRI \n",
    "\n",
    "Functional Magnetic Resonance Imaging, or fMRI, is a measure of brain activity over time. FMRI data is acquired using a special pulse sequence designed to measure changes in the magnetic properties of the blood flow using the Magnetic Resonance Imaging (MRI) scanner.\n",
    "\n",
    "![alt text](../../data/images/mri_scanner.png \"mri_scanner\")\n",
    "\n",
    "\n",
    "The functional signal we measure with fMRI is *not* an electrical neural signal (as in EEG, ECoG, or electrophysiology). It is a magnetic signal related to the properties of brain tissue, and it is dominated by blood flow. Blood flow is related to neural activity, because firing neurons need oxygen. The process of firing involves letting electrically charged ions into a cell and actively pumping them back out again, which is metabolically demanding. So once a region of the brain becomes active (once the neurons start firing), metabolism in that region is high, oxygen gets stripped off of hemoglobin molecules in red blood cells in the area (thereby changing the magnetic properties of hemoglobin, creating a deoxyhemoglobin). This initiates a complex process to increase blood flow to the electrically active area. \n",
    "\n",
    "![alt text](../../data/images/deoxyhemoglobin.png \"deoxyhemoglobin.png\")\n",
    "\n",
    "The specific mechanisms that lead from neural activity to changes in blood flow are (a) not well understood, and (b) beyond the scope of this class. For now, just know that there are several ways to measure functional responses with MRI, and the specific one that we work with is the Blood Oxygenation Level Dependent Response, or the BOLD response. \n",
    "\n",
    "### fMRI has high spatial but low temporal resolution\n",
    "\n",
    "We have belabored this here a little because this complexity should always be a source of humility for anyone working with fMRI or trying to interpret fMRI results. It is an **indirect, slow measure** - and these considerations strongly constrain the kinds of experiments you can do with fMRI and the conclusions you can draw from those experiments.\n",
    "\n",
    "An important practical upshot of this for our purposes is that the signal we measure changes much more slowly than the signal we measure in EEG or ECoG. Responses emerge over seconds, not milliseconds.\n",
    "\n",
    "![alt text](../../data/images/imaging_modalities.png \"imaging_modalities.png\")\n",
    "\n",
    "                                      Sejnowski et al., Nature Neurosci., 2014\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "One fMRI image (fMRI volume) is acquired for a given unit of time called a repetition time (TR). A TR is typically 1-2 seconds *(usually 1.0, 1.5, or 2.0 seconds)*. Every image records the activity in the brain at a given point in time. The following image shows a single volume of fMRI data (one two-second snapshot of brain activity).\n",
    "\n",
    "![alt text](../../data/images/fig1slice.png \"fig1\")\n",
    "\n",
    "The dimensions of the brain volume measured by fMRI can vary. Each individual fMRI measurement unit is called a *voxel*, which is short for volumetric pixel. The voxels in this data are about 2.4 x 2.4 x 4.0 mm (X x Y x Z) in size. \n",
    "\n",
    "### FMRI activity in time\n",
    "\n",
    "Once a neural event is triggered by a stimulus presentation the vascular system needs to respond to the need for glucose and oxygen in that specific brain area. This can take up to 1-2 seconds. Hence the hemodynamic response lags the triggered event by 1-2 seconds, which peaks around 5 seconds after the stimulus onset.\n",
    "\n",
    "![alt text](../../data/images/lagged_activity.png \"lagged_activity.png\")\n",
    "\n",
    "### Example fMRI experiments \n",
    "\n",
    "#### Viewing natural images \n",
    "\n",
    "![alt text](../../data/images/fmri_example_experiment.png \"fmri_example_experiment.png\")\n",
    "\n",
    "#### Fusiform Face Area (FFA)\n",
    "\n",
    "![alt text](../../data/images/ffa.png \"ffa.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FMRI as a *mapping* technique\n",
    "The scanning volume for the data we will use here consists of 30 transverse slices (i.e. Z is between 0 and 29). Transverse slices are horizontal, i.e., approximately parallel to the plane of the eyes and ears. Each slice corresponds to a 2D image of 100 x 100 voxels.\n",
    "\n",
    "We can measure fMRI responses across the whole brain. Therefore fMRI can be seen as a *mapping* technique. In order to make brain maps, we have to be able to match measured voxel responses to the participant's brain anatomy. Using a different type of MRI sequence, we can collect a **structural scan** (also called **anatomical scan**), and obtain a high resolution image of a participant's brain.\n",
    "\n",
    "\n",
    "![alt text](../../data/images/MPRAGE.png \"MPRAGE.png\")\n",
    "\n",
    "![alt text](../../data/images/MPRAGE_wcortex.png \"MPRAGE_wcortex.png\")\n",
    "\n",
    "Further, fMRI studies often focus on the cerebral cortex (the outermost layer of the brain). Consequently, it is common to display the results of statistical analyses of fMRI data on inflated and flattened representations of the cerebral cortex. Such cortical surface maps provide a way to examine all cortical fMRI data at once, with the anatomical location of the functional data made clear. \n",
    "\n",
    "The cortical surface must be computationally extracted from high spatial resolution anatomical MRI scans, and often manually edited (*NOTE: Manual editing to create a good corical surface can take days or weeks of effort! This data is not free!*)\n",
    "\n",
    "![alt text](../../data/images/cortex_3views.png \"cortex_3views.png\")\n",
    "\n",
    "\n",
    "\n",
    "## Storing fMRI data for data analysis\n",
    "\n",
    "We store fMRI data as a matrix. This means that each volume (a timepoint) in the experiment will correspond to a row in the matrix, and each voxel will correspond to a column in that matrix. For this reason, we need to make sure the criteria we use to move each 3D image to a matrix row is preserved and this operation is inverted. Let's look at an example.\n",
    "\n",
    "![alt text](../../data/images/fmri_dimensions.png \"fmri_dimensions.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fMRI data is stored in a variety of formats. The medical imaging community has a standardized image format called **Digital Imaging and Communications in Medicine (DICOM)** to handle and store raw medical imaging data. When data is collected using the MRI scanner each volume is stored in a DICOM file that contains both a header and the image data. The DICOM header stores useful information about the participant's name, pulse sequence, the type of scan, image dimensions, etc. (Another popular medical imaging format is the Analyze format, where the image and header file are stored separately.) You can read about these file formats [here](http://people.cas.sc.edu/rorden/dicom/index.html).\n",
    "\n",
    "Before we start analyzing the data we convert the raw DICOM files into the commonly used **Neuroimaging Informatics Technology Initiative (nifti)** file format. Files stored in this format usually have the extension .nii or .nii.gz. \n",
    "\n",
    "We will use the `nibabel` python module to load data that is stored such data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import nibabel\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt  # for visualization\n",
    "\n",
    "%matplotlib widget\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will load one run (also referred to as a scan) worth of fMRI data that was stored as a nifti file format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a nifti (nii) proxy object\n",
    "fname = '../../data/fMRI/categories/s01_categories_01.nii.gz'\n",
    "nii = nibabel.load(fname) \n",
    "\n",
    "# This object stores the infomation *about* the fMRI data stored in the file. \n",
    "# This meta-data can be accessed via attributes of the `nii` object.\n",
    "print('nii.in_memory : ', nii.in_memory)\n",
    "print('nii.shape : ', nii.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve actual data as an array\n",
    "data = nii.get_fdata()\n",
    "print('nii.in_memory : ', nii.in_memory)\n",
    "print('data shape : ', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions of the data are (X, Y, Z, T) (T is time, in TRs). Thus, there are 120 volumes (120 time points). Each volume has 30 horizontal or transverse slices with 100 x 100 pixels.\n",
    "\n",
    "![alt text](../../data/images/slices.png \"slices.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose data\n",
    "\n",
    "When we work with functional images it is in general more convenient (for reasons like averaging over time, transfering data to a standard units, etc.) to have the data in T, Z, Y, X format. This is the opposite of the conventions you've seen so far for EEG / ECoG data. \n",
    "\n",
    "When we worked with EEG/ECoG data in the first half of the class, time was the *last* axis; here, time is the *first* axis. The reasons for this convention will become more obvious as we go, and we see how this convention makes for convenient syntax and shortcuts. \n",
    "\n",
    "Hence, we will use the `transpose` function  of the numpy package (or `.T` method of numpy arrays) to make this dimensional switch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Transpose the data\n",
    "print(data.shape)\n",
    "dataT = np.transpose(data)\n",
    "\n",
    "# Or, equivalently\n",
    "dataT_ = data.T\n",
    "\n",
    "print(\"dataT shape :\", dataT.shape)\n",
    "print(\"dataT_ shape : \", dataT_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assert function\n",
    "You can check whether to variables are the same using the `assert` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "a = 2 \n",
    "b = 1\n",
    "assert a==b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also append an error message. This message will be printed if the two are not equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "assert a==b, 'these two are not equal!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout session: Explore the function transpose\n",
    "\n",
    "> Now test whether dataT and dataT_ are the same using assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Are dataT and dataT_ the same?\n",
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "\n",
    "For fMRI, the functions we will use to explore the data are in general more basic than the functions we used in the first part of the class. We will not use a single module like MNE to make standard plots; we will construct our own!\n",
    "\n",
    "This is a little more work, but is very generalizable to other kinds of analyses, and if you get good at it you can make exactly the plot of your data that you want.\n",
    "\n",
    "One of the first questions about a data set (after its size) that you should explore is \n",
    "\n",
    "    \"What is the scale (min/max) of the data?\". \n",
    "\n",
    "Afterwards you may be interested to know its mean, standard deviation, and how in general the data looks like and make a histogram plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Keep data transposed\n",
    "data = dataT\n",
    "\n",
    "del dataT, dataT_\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Find out the scale of the data\n",
    "print(np.min(data), ', ', np.max(data)) \n",
    "\n",
    "# Find out its mean\n",
    "print(np.mean(data))\n",
    "\n",
    "# Find out its mean\n",
    "print(np.std(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Make a histogram of the data\n",
    "print(data.shape)\n",
    "print(data.flatten()[:10])\n",
    "_ = plt.hist(data.flatten(), bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> - What does this tell you about the data? \n",
    "\n",
    "> - What are the axes on this plot? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the timecourse of a single voxel\n",
    "Now we can plot the timecourse for one voxel somewhere in the middle of the brain (e.g. at Z=10, Y=34, X=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "_ = plt.plot(data[:, 10, 34, 34])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout session:\n",
    "> - Save all transverse slices for a sagittal and coronal axis (you can choose any number), in a variable called `transverse``\n",
    "\n",
    "> - Make a plot for all transverse slices (different voxels are plotted in the same figure). Label the axis. What does this plot tell us?\n",
    "\n",
    "> - Transpose the transverse variable and plot. Label the axis accordingly. What does this plot tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have 30,000 measurements to plot like this. So we *could* make a plot like the eeg representations that we had, but those were pretty busy even with 60 lines. So, instead, we will view our data as images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying data as an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will get a broader view of the first volume of our data. The (T, Z, Y, X) dimension ordering that we have for the data makes it easy to select volumes (time snapshots of brain activity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some ways to select volumes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can select one volume like this: \n",
    "first_volume = data[0, :, :, :]\n",
    "\n",
    "# Or like this: \n",
    "alt_first_volume1 = data[0, ...]\n",
    "\n",
    "# Or like this: \n",
    "alt_first_volume2 = data[0]\n",
    "\n",
    "# These are all the same1\n",
    "assert np.all(first_volume==alt_first_volume1)\n",
    "assert np.all(first_volume==alt_first_volume2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(first_volume.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the horizontal slice\n",
    "Let's look at an example of a horizontal slice from the first volume. This can be done by selecting one of the slices as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Z=15 is halfway through the volume we have scanned\n",
    "slice_horizontal = first_volume[15,:,:]\n",
    "\n",
    "# You can set the image origin [0,0] to be in the lower left corner\n",
    "# by using origin='lower'\n",
    "plt.figure()\n",
    "im = plt.imshow(slice_horizontal, origin='lower') \n",
    "_ = plt.colorbar(im)\n",
    "plt.show()\n",
    "\n",
    "# Alternative:\n",
    "plt.figure()\n",
    "im = plt.imshow(slice_horizontal[::-1]) \n",
    "_ = plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout session:\n",
    "> - Plot other slices to see how the shape of the brain is different\n",
    "> - Change the properties of the figure. Explore the keyword arguments for imshow, see what each does! (hints: show axes, change colormap, what about vmin and vmax values, set those)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colormaps: https://matplotlib.org/stable/users/explain/colors/colormaps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "# You can set the image origin [0,0] to be in the lower left corner\n",
    "# by using origin='lower'\n",
    "plt.figure()\n",
    "im = plt.imshow(slice_horizontal, origin='lower', cmap='inferno', vmax=1500) \n",
    "_ = plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout session:\n",
    "> Write a small helper function that takes a slice number as an input returns the data (2D array) of that slice\n",
    "> Plot this image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "def select_slice(slice_number):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_number = 12\n",
    "\n",
    "plt.figure()\n",
    "im = plt.imshow(select_slice(slice_number), origin='lower', cmap='inferno') \n",
    "_ = plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout session: Visualizing sagittal and coronal views\n",
    "We can also slice the brain on different axis to obtain sagittal and coronal slices. \n",
    "\n",
    "> Try to plot these below. We have not told you which dimension corresponds to slicing the brain in a sagittal or coronal view. You should try the different axes and find it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Select a sagittal slice and plot\n",
    "### STUDENT ANSWER\n",
    "def select_slice_coronal(slice_number):\n",
    "    pass\n",
    "    \n",
    "slice_number = 45\n",
    "\n",
    "plt.figure()\n",
    "im = plt.imshow(select_slice_coronal(slice_number), origin='lower', cmap='inferno') \n",
    "_ = plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Select a coronal slice and plot\n",
    "### STUDENT ANSWER\n",
    "\n",
    "def select_slice_sagittal(slice_number):\n",
    "     pass\n",
    "    \n",
    "slice_number = 70\n",
    "\n",
    "plt.figure()\n",
    "im = plt.imshow(select_slice_sagittal(slice_number), origin='lower', cmap='inferno') \n",
    "_ = plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing matplotlib default parameters\n",
    "\n",
    "You can set the default colormap, default interpolation or many other parameters in `matplotlib.rcParams`.\n",
    "\n",
    "For example to set all the colormaps in this `ipython` session to the colormap 'viridis' we can use the following line:\n",
    "    * `matplotlib.rcParams['image.cmap'] = 'viridis'` # or whatever your favorite map is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['image.cmap'] = 'viridis' # or whatever your favorite map is e.g. 'gray', 'hot'\n",
    "matplotlib.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "An alternative way to change a figure's properties is to create a dictionary of keywords that can be used as a  keyword argument to the `imshow` function.\n",
    "\n",
    "This has the advantage of not setting the default parameters. Yet, we can easily change a number of parameters in the `imshow` function by just passing the keywords dictionary to the `imhsow` function. The following cell is demonstrating this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "im_kws = dict(aspect='auto', vmin=0, vmax=2000, cmap='hot', interpolation='nearest') \n",
    "plt.imshow(first_volume[:,  30, :], **im_kws)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all horizontal slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to make a plot with all of the horizontal slices, so we can see one entire 3D volume at once. For this, we will use the `subplot()` function in matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,10))\n",
    "n_slices = 30\n",
    "nrows, ncols = 5, 6\n",
    "for s in range(n_slices):\n",
    "    ax = fig.add_subplot(nrows, ncols, s+1)\n",
    "    slice_horizontal = first_volume[s,:,:]\n",
    "    plt.imshow(slice_horizontal)\n",
    "    ax.axis('off')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a function that plots the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_horizontal_slices(vol, **kwargs):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    nslices = vol.shape[0]\n",
    "    subplot_size = int(np.ceil(np.sqrt(nslices)))\n",
    "    for s in range(nslices):\n",
    "        ax = fig.add_subplot(subplot_size, subplot_size, s+1) \n",
    "        slice_horizontal = vol[s,:,:]\n",
    "        plt.imshow(slice_horizontal, **kwargs)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout session:\n",
    "> Call the above function (plot_horizontal_slices), and try out different plotting arguments (e.g. change the colormap, change the interpolation, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "You may have noticed in the figure above that many of the voxels do not overlap with the brain (or more specifically the gray matter in the cortex) at all. Actually, let's try to plot some of those voxels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(data[:,0,0,0], label='Edges of volume, 1')\n",
    "plt.plot(data[:,1,1,1], label='Edges of volume, 2')\n",
    "plt.plot(data[:,-1,-1,-1], label='Edges of volume, 3')\n",
    "\n",
    "# A middle brain voxel\n",
    "plt.plot(data[:,10,34,34], label='Mid brain')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edge-of-scan voxels are clearly not in the brain, however, they show some variance due to noise. It is the practice in fMRI to mask out, or zero out these voxels.\n",
    "\n",
    "A mask is a 3D binary array that is derived from the high resolution anatomical scan of the subject. The mask indicates, for every voxel in the 30 x 100 x 100 matrix, which ones should be ignored and which should be kept. Let's load and look at a voxel mask for this subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load a stored mask for this subject\n",
    "fname = '../../data/fMRI/categories/s01_categories_mask_cortical.npz'\n",
    "mask = np.load(fname)['mask']\n",
    "print('Mask shape: ', mask.shape)\n",
    "print(mask[10,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot one fo the slices below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "mask_horizontal = mask[15,:,:]\n",
    "plt.imshow(mask_horizontal, origin='upper', cmap='gray')\n",
    "_ = plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mask clearly indicates which voxels should be kept. \n",
    "\n",
    "### Breakout session: \n",
    "> Let's look at the 3D structure. Use the function we defined above to plot the entire 3D mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how now we have a mask that indicates which voxels we should keep and which we should exclude.\n",
    "\n",
    "We can use the mask to set the tiny values outside the brain, and the values in the middle of the brain (in subcortical stuctures) to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the horizonral slices for the first volume\n",
    "plot_horizontal_slices(first_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "masked_vol_0 = first_volume * mask\n",
    "\n",
    "# Now plot masked_vol_0:\n",
    "plot_horizontal_slices(masked_vol_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the voxels outside the cortex (including in the middle of the brain) are zeroed out now! Let's apply this mask to the entire dataset now and zero out the same voxels in each volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create an array with zeros\n",
    "print(data.shape)\n",
    "masked_data = np.zeros_like(data)\n",
    "\n",
    "# NOTE: np.zeros_like is similar to creating an array using np.zeros. We can check this by:\n",
    "assert np.all(np.zeros(data.shape)==np.zeros_like(data)) \n",
    "\n",
    "# Not run through the entire data set and mask each volume\n",
    "for i in range(data.shape[0]):\n",
    "    masked_data[i,:,:,:] = data[i,:,:,:] * mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace the above foor loop using list comprehension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "first_volume = data[0]\n",
    "print('shape of first_volume is {}'.format(first_volume.shape))\n",
    "\n",
    "first_volume_masked = first_volume[mask==True]  # we can omit ==True, because mask is already a boolean array\n",
    "print('shape of first_volume_masked is {}'.format(first_volume_masked.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "_ = plt.hist(first_volume_masked.flatten(), 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compress the entire matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "masked_data_v3 = data[:, mask]\n",
    "\n",
    "print('masked_data_v3 is a {0} of size {1}'.format(type(masked_data_v3), masked_data_v3.shape))\n",
    "print('each element is a {0} of size {1}'.format(type(masked_data_v3[0]), masked_data_v3[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make another histogram of the masked (collapsed) data, we can see that we have a more interesting range of values left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "_ = plt.hist(masked_data_v3.flatten(), 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily go back and forth between the two formats, we need to have the mask and the 2D matrix to produce the 4D matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "unmasked_data = np.zeros(data.shape)\n",
    "unmasked_data[:, mask] = masked_data_v3 \n",
    "\n",
    "print('unmasked_data is a {0} of size {1}'.format(type(unmasked_data), unmasked_data.shape))\n",
    "print('each element is a {0} of size {1}'.format(type(unmasked_data[0]), unmasked_data[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "_ = plt.hist(unmasked_data.flatten(), 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Time series\n",
    "\n",
    "Remember that one of these volumes is acquired at every time unit. The time unit here is 2.0045 seconds. Let's look at one slice at different time points. Now because masked_data_v3 only has cortical voxels, we can plot any of its dimension knowing that we are looking at gray matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "masked_data_v3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the activity in time for different voxels. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "TR = 2.0045\n",
    "n_points = 100\n",
    "time_points = np.arange(n_points)*TR\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(time_points, masked_data_v3[:n_points, 4].T)\n",
    "_ = plt.xlabel(\"Time (s)\")\n",
    "_ = plt.ylabel(\"fMRI activity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot four different voxels time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(time_points, masked_data_v3[:n_points, 4].T)\n",
    "plt.plot(time_points, masked_data_v3[:n_points, 10].T)\n",
    "plt.plot(time_points, masked_data_v3[:n_points, 100].T)\n",
    "plt.plot(time_points, masked_data_v3[:n_points, 1000].T)\n",
    "\n",
    "_ = plt.xlabel(\"Time (s)\")\n",
    "_ = plt.ylabel(\"fMRI activity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voxels seem to have a different baseline! (You might have suspected this based on the image plots or the histogram plots above, too).\n",
    "\n",
    "Let's plot one slice at different time points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,10))\n",
    "z_slice = 10\n",
    "for s in range(15):\n",
    "    ax = fig.add_subplot(5, 5, s+1)\n",
    "    slice_horizontal = data[s, z_slice, :, :]\n",
    "    slice_horizontal[~mask[z_slice, : , :]] = 0\n",
    "    plt.imshow(slice_horizontal)\n",
    "    plt.title('TR #{n}'.format(n=s+1))\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.imshow(masked_data_v3, aspect='auto')\n",
    "# Set the axis labels using the `setp` method.\n",
    "# http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.setp\n",
    "plt.setp(ax, xlabel='Voxels', ylabel='Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the activity at each voxel (zscore across time)\n",
    "\n",
    "You can see the same effect in the time plots as well as in the mosaic plots: some voxels have a different baseline than others. \n",
    "\n",
    "We need to normalize the activity of each voxel in time to be able to see local fluctuations in the signal. This normalization is also called *z-score* or *standard score*.\n",
    "\n",
    "1. We will first take the mean and standard deviation across time for each cortical voxel.\n",
    "2. For each voxel, we will substract the mean from each time point.\n",
    "3. For eacl voxel, we will divide each time point by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(masked_data_v3.shape)\n",
    "data_norm = masked_data_v3 - masked_data_v3.mean(axis = 0)\n",
    "data_norm = data_norm / data_norm.std(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the time course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "TR = 2.0045\n",
    "points = range(0,100)\n",
    "time_points = np.array(points)*TR\n",
    "\n",
    "plt.plot(time_points, data_norm[:n_points, 4].T)\n",
    "plt.plot(time_points, data_norm[:n_points, 10].T)\n",
    "plt.plot(time_points, data_norm[:n_points, 100].T)\n",
    "plt.plot(time_points, data_norm[:n_points, 1000].T)\n",
    "\n",
    "_ = plt.xlabel(\"Time (s)\")\n",
    "_ = plt.ylabel(\"fMRI activity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.imshow(data_norm, aspect='auto') \n",
    "plt.setp(ax, xlabel='Voxels', ylabel='Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "And plot the volumes in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_norm_unmasked = np.zeros_like(data)\n",
    "data_norm_unmasked[:,mask] = data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,10))\n",
    "for s in range(15):\n",
    "    ax = fig.add_subplot(5,5,s+1)\n",
    "    slice_horizontal = data_norm_unmasked[s,10,:,:]\n",
    "    slice_horizontal[~mask[10,:,:]] = 0\n",
    "    plt.imshow(slice_horizontal, vmin=-3, vmax=3) # don't forget vmin / vmax!\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization using pyCortex\n",
    " \n",
    "We will now work with a visualization tool called [pycortex](https://gallantlab.org/pycortex/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Alignment\n",
    "fMRI measures activity across the whole brain (or large swathes of the brain), but it is difficult to map that activity to specific anatomical locations in the brain based on functional data alone. Many factors make anatomy ambiguous in functional data: **(1)** functional data are low resolution; **(2)** functional slices are not always aligned with the head in a consistent or precise way across subjects; and **(3)** there is considerable variation in anatomy (i.e., specific locations of sulci & gyri) across subjects. Thus, a common step in fMRI processing is to align the functional data with a high-resolution anatomical scan of the same subject. \n",
    "\n",
    "### Alignment in principle\n",
    "The scanner stores the location of the slices with respect to the magnet *isocenter* (the strongest point of the magnetic field in the MRI scanner; the center of the magnet). This data is transferred to the nifti files at file creation time. These transformations (from data space to scanner space) are still not enough to precisely align data from different anatomical and functional scans. The functional and anatomical data may have been collected on different days, or the subject may have shifted slightly between scans. Thus, the functional data needs to be moved and/or rotated in 3D to make sure it aligns as precisely as possible with the underlying anatomical data. \n",
    "\n",
    "\n",
    "![alt text](../../data/images/fMRI_Transforms.001.png \"fMRI_Transforms.001.png\")\n",
    "\n",
    "\n",
    "### Alignment in pycortex\n",
    "pycortex allows manual adjustment of the rotation & position of the functional data with respect to the 3D surface.\n",
    "\n",
    "![alt text](../../data/images/pycortex_aligner_transverse.png \"pycortex_aligner_transverse.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some necessary libraries\n",
    "import os\n",
    "import nibabel  # Neuroimaging library to work with MR images\n",
    "import cortex  # This is our mapping software pycortex\n",
    "# import neurods # Our class module\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set matplotlib defaults\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.aspect'] = 'auto'\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "\n",
    "%matplotlib widget\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Same as previously: load using nibabel, use get_data() method of the nibabel data object, transpose resulting data array, and zscore the data. For now, we will treat this as a generic data set (next week, we will learn more about the experiment that generated this data as we begin to actually analyze the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '../../data/fMRI/categories/s01_categories_01.nii.gz'\n",
    "nii = nibabel.load(fname) \n",
    "voldata = nii.get_fdata().T\n",
    "\n",
    "mask_fname = '../../data/fMRI/categories/s01_categories_mask_cortical.npz'\n",
    "mask = np.load(mask_fname)['mask']\n",
    "data = voldata[:, mask]\n",
    "data = zscore(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_volume = voldata[0]\n",
    "print(\"Dimensions of the first volume: {0}\".format(first_volume.shape))\n",
    "print(\"Dimensions of the data: {0}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is: (time, Z, Y, X). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onward to 3D data visualizations! \n",
    "\n",
    "We will use a python module called pycortex to show data in 3D on the brain. This module was developed here at UC Berkeley in the Gallant lab, mostly by James Gao, with help from Alex Huth, Mark Lescroart, and other lab members. The code is freely available online [here](https://github.com/gallantlab/pycortex), and a paper summarizing the code can be found [here](http://journal.frontiersin.org/article/10.3389/fninf.2015.00023/full). \n",
    "\n",
    "To map the functional data onto the cortex, pycortex requires at least two things:\n",
    "\n",
    "1. The cortical surface of the subject. \n",
    "    * pycortex stores cortical surface files (and several other files) for each subject in a reliably structured directory of files. Because of this reliable directory structure, all we need to provide to the code is a subject ID string, and the code will be able to find and load the relevant cortical surface files. \n",
    "2. The functional to anatomical aligmnent of this data to that cortical surface\n",
    "    * Alignment of functional data to anatomical data proceeds by an *affine transform*. How this transformation works is beyond the scope of this class, but you can look it up on [wikipedia](https://en.wikipedia.org/wiki/Affine_transformation) or in your favorite linear algebra textbook if you're curious. Or here: https://andysbrainbook.readthedocs.io/en/latest/fMRI_Short_Course/Preprocessing/Registration_Normalization.html\n",
    "    * The practical upshot is that a 4x4 matrix of numbers is sufficient to store the 3 rotations (around the x, y, and z axes) and 3 the transformations (in the x, y and z dimensions) that will transform the functional data in space such that they are aligned with the anatomical data (with the cortical surface). In the pycortex code, \"transform\" is abbreviated in variable names as `xfm`. Just as with the cortical surface, we only need to specify a name for a transform, and the code will know where to find the file that contains the affine transformation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) subject (specifies the cortical surface of the brain)\n",
    "subject = 's01' \n",
    "# (2) transform \n",
    "transform = 'catloc' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display data in pycortex, we need to create a pycortex object that contains all the relevant information: the data, the cortical surface (i.e., the subject) and the affine transform. That object is called a Volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a volume\n",
    "data_volume = cortex.Volume(first_volume, subject, transform) \n",
    "print(data_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Show the volume on a flatmap (ooooh)\n",
    "# cortex.webgl.show(data_volume)\n",
    "_ = cortex.quickflat.make_figure(data_volume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Show a flatmap inline\n",
    "# (If you get warnings about a module called shapely, ignore them; they are not important.)\n",
    "_ = cortex.quickflat.make_figure(data_volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masks\n",
    "pycortex can be used to generate cortical masks; this is where the mask we used above came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mask = cortex.db.get_mask(subject, transform, type='cortical')\n",
    "first_volume_masked = voldata[0, mask]\n",
    "print('Shape of mask: ', mask.shape)\n",
    "print('Shape of masked data: ', first_volume_masked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pycortex can also directly visualize masked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show creation of volume from masked data\n",
    "print(data.shape)\n",
    "kwargs = dict(vmin=-3, vmax=3)\n",
    "vol = cortex.Volume(data[0], subject, transform, mask=mask, **kwargs)\n",
    "print(vol)\n",
    "print(vol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display this volume, too\n",
    "_ = cortex.quickflat.make_figure(vol)\n",
    "# Select other timepoints to display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a movie volume\n",
    "data_movie = cortex.Volume(data, subject, transform, mask=mask, vmin=-3, vmax=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### TEACHER INFO\n",
    "# show 3D web display with a 4D volume\n",
    "# cortex.webgl.show(data_movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

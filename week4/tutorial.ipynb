{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Now we've gotten a feel for what raw EEG / ECoG data look like, and we've successfully sliced up that data according to events that we detected in the neural signals (eyeblinks). However, what we're really interested in is to determine how the neural activity corresponds to events **in the world**. In this class we will begin exploring how to conduct experiments in order to ask questions about the function of the brain. We'll discuss how to use event timing information in order to extract patterns of activity that correspond to that event. We'll also discuss how to compare these patterns between event types.\n",
    "\n",
    "## Goals for today\n",
    "* Understanding how an experiment relates to the neural data / how to use metadata in analyses.\n",
    "* Create an `Epochs` representation of data. That is, create a window around times of interest and analyze patterns of activity in that window.\n",
    "* Infer differences in activity between events, and use statistics / resampling methods to determine if there is a \"significant\" difference between conditions.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Experiments\n",
    "Now that we know how to slice up our data, we can begin running experiments! Thus far, we've focused on defining \"events\" as physiological signals that we can detect automatically (e.g., eyeblinks). However, in neuroscience it is most common to keep information about *when* we presented certain things to the brain, and use this to ask *what* happened as a result.\n",
    "\n",
    "For this, we should talk a bit about experiments. See the lecture slide today for a quick primer on experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import datascience as ds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import neurods as nds\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "path_raw_data = '../../data/eeg/mne_sample/mne_sample-raw.fif'\n",
    "raw = mne.io.Raw(path_raw_data)\n",
    "\n",
    "# Load the events\n",
    "path_events = '../../data/eeg/mne_sample/mne_sample-events.csv'\n",
    "ev_df = ds.Table.read_table(path_events)\n",
    "\n",
    "# Load the event mapping\n",
    "path_event_info = '../../data/eeg/mne_sample/mne_sample-event_info.csv'\n",
    "ev_info = ds.Table.read_table(path_event_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at the events\n",
    "ev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Event id mappings\n",
    "ev_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Column labels\n",
    "print(ev_df.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we'll load in the event timings\n",
    "# We need to reshape them to be the format that MNE wants\n",
    "events = ev_df.select(['index', 'previous_value', 'event_type']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ev_df.values[:5])\n",
    "print('\\n---\\n')\n",
    "# Note that it's the same data, but with columns rearranged\n",
    "print(events[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also use our event info to create a dictionary MNE can use\n",
    "ev_id = dict()\n",
    "for i_name, i_id, i_desc in ev_info.rows:\n",
    "    ev_id[i_name] = int(i_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ev_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This lets us create an Epochs object, which we can use to view the data\n",
    "epochs = mne.Epochs(raw, events, event_id=ev_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we wanted to operate on the data directly, we would not be able to access `_data` yet because `preload=False` when we constructed the epochs. In this case, to get the data we can use the `get_data` method, which is a way of loading the data in case `preload` is `False`. It will simply return an array of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(epochs.get_data().shape)\n",
    "n_timepoints = epochs.get_data().shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can take a look at the raw data and see what happens when an event starts:\n",
    "_ = epochs.plot(scalings='auto', n_epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> * Can you tell any differences between the event types?\n",
    "> * How is this different / similar to the eyeblink events?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-Related Potentials (ERPs)\n",
    "Signals such as EEG are often too noisy to see something going on on a single trial level. There is too much variability from trial to trial, and this variability is often a larger amplitude than the signal we care about.\n",
    "\n",
    "However, this variability is also semi-random, whereas the brain response to an event is consistent. This means that we should be able to \"average out\" the noise, and retain the signal.\n",
    "\n",
    "To do this, we generally average across multiple instances of the same event. The noise is random, so tends to cancel itself out, while the signal should remain.\n",
    "\n",
    "In general, EEG or ECoG will have a similar response to the same stimulus. When we average out all the noise across trials, we are left with these \"average\" responses. We call them *event-related potentials* (ERPs).\n",
    "\n",
    "Below we'll calculate some ERPs for a particular event-types and see how they compare with one another. We'll focus on the events where a person pressed a button here, and we'll go through some other event types in the lab.\n",
    "\n",
    "First, we need to figure out which `event_id` corresponds to a button press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First check what epochs is\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll take a look at what happened when the person pressed a button\n",
    "# First find the event IDs to determine the type we want\n",
    "epochs.event_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to pull the subset of epochs that correspond to this event type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, we can pull the subset of events corresponding to a button press:\n",
    "epochs_button = epochs['button']\n",
    "\n",
    "# Note that we could also just index a subset of events like this:\n",
    "#epochs[2:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the raw data to see if there is a particular pattern that emerges for this kind of trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at the raw data again to see what's there\n",
    "_ = epochs_button.plot(scalings='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch_data = epochs_button.get_data()\n",
    "epoch_data.shape # (n_events, n_channels, n_timepoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll average across trials for each electrode now, and see if something clear comes out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll average across events/trials by using the MNE `average` method.\n",
    "evoked = epochs_button.average()\n",
    "_ = evoked.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> * Does it seem like there is something going on here? Is it more or less noisy than the eyeblinks we looked at before?\n",
    "\n",
    "Let's investigate further by plotting the topomap at different points in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can clean up this figure a little bit by using the `plot_joint` function\n",
    "_ = evoked.plot_joint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> * What pattern emerges across the brain for this event type?\n",
    "> * Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselining with Events\n",
    "Last week, we talked about using baselining to make sure that each channel was at roughly the same level, this made it easier to compare activity between channels.\n",
    "\n",
    "However, there's another important use for baselining. Often in neuroscience we care about the brain's response to an event (e.g. the onset of a sound). As such, we really want to know what the response was like *after* the event occurred, relative to the brain activity *before* the event occurred.\n",
    "\n",
    "Look at the output of the `print` statement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(epochs_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that they include `baseline : (None, 0)`. This means that each epoch has been baselined using all pre-stimulus times (`None` just means \"take the earliest possible timepoint for a given window, and `0` is the event onset).\n",
    "\n",
    "We can re-baseline the data using other timepoints, let's see how this changes the plot for the average activity. \n",
    "\n",
    "We can re-baseline data with the `mne.baseline.rescale` function. This takes data as input, as well as a time window for a baseline, and it will subtract the mean within the baseline window *for every channel, for every epoch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mne.Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll preload the data so it's available to work on\n",
    "epochs.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We'll try a few baselines and plot the results of each\n",
    "baselines = [(-.5, 0), (0, .05), (.05, .1), (.1, .15)]\n",
    "for baseline in baselines:\n",
    "    # Using `copy=False` modifies the data in-place\n",
    "    _ = mne.baseline.rescale(epochs._data, epochs.times, baseline, copy=False)\n",
    "    av = epochs.average()\n",
    "    fig = av.plot(show=False)\n",
    "    fig.axes[0].set_title('Baseline: {}'.format(baseline))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Field Potential\n",
    "\n",
    "As we've seen, there's a lot of interesting detail in how the activity changes across channels (e.g. looking at topomaps). However, sometimes we just want a single timeseries that describes the general activity across the *entire* collection of channels. This basically removes information about each individual channel, and instead describes at a general level when things are happening.\n",
    "\n",
    "The simplest way to do this might be to average across all of the electrodes...let's try that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll perform this manually on the epochs data\n",
    "data = epochs_button.get_data()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, average across each trial:\n",
    "data_av = np.mean(data, 0)  # same as above: epochs_button.average()\n",
    "\n",
    "# Then, average across channels\n",
    "data_global = np.mean(data_av, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(data_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We'll plot the general activity level for this subject with matplotlib:\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs.times, data_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> * You might notice that the y-axis is really tiny...why is this?\n",
    "> * Does a clear picture seem to emerge from this plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try squaring each channel first, and then see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_global = np.mean(data_av ** 2, 0)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs.times, data_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we can see that there is a clear peak that occurs just after time 0, and maybe another after 100ms.\n",
    "\n",
    "> * What kind of information do you think we can infer from this?\n",
    "> * Why would there be two different peaks in the signal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we square each channel, and then average across channels, we calculate something called the *Global Field Potential*. This is a general description of when there is \"stuff going on\" across the entire collection of electrodes.\n",
    "\n",
    "# Calculating / Visualizing variability\n",
    "In this lecture, we've averaged out the \"noise\" present from trial to trial by averaging across trials, assuming that whatever remains is the \"signal\" we really care about. However, it is often important to know more than just the \"mean\" signal across trials. It's also useful to know how *variable* the signal is across trials.\n",
    "\n",
    "There are many ways to calculate variability across trials, but the easiest is probably *standard error*. Briefly, this is a way for expressing how *confident* we are in the mean that is calculated across trials. For example, these two signals are sampled from the same univariate “normal” (Gaussian) distribution with mean 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sig_a = 6 * np.random.randn(1000)\n",
    "sig_b = 2 * np.random.randn(1000)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(sig_a)\n",
    "ax.plot(sig_b)\n",
    "\n",
    "print(sig_a.mean())\n",
    "print(sig_b.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we can plot the mean of each signal as a scatterplot\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter([0, 1], [sig_a.mean(), sig_b.mean()])\n",
    "ax.set_ylim([-.5, .5])\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Signal A', 'Signal B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can tell that *just* calculating the mean for each signal will miss something important about the difference between the two. We need some measure about the *variability* of the mean, which we can also describe as our *uncertainty* of the \"true\" mean. Let's do this with standard error, which takes the following form:\n",
    "\n",
    "$$sterr = \\frac{\\sigma}{\\sqrt{N}}$$\n",
    "\n",
    "That is, we calculate the standard deviation ($\\sigma$) of each signal, then divide it by the square root of the number of samples ($N$). This means that the more samples we have, the smaller the standard error will be, and the more confident we are that the sample mean is the \"true\" mean.\n",
    "\n",
    "By calculating standard error, our scatterplot above can be a line plot, where the limits of the line express the standard error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard_error(data):\n",
    "    return np.std(data) / np.sqrt(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for ii, sig in enumerate([sig_a, sig_b]):\n",
    "    ax.plot(ii, sig.mean(), 'og')\n",
    "    ax.plot([ii, ii],\n",
    "            [sig.mean() - standard_error(sig),\n",
    "             sig.mean() + standard_error(sig)], 'b')\n",
    "ax.set_xlim([-1, 2])\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Signal A', 'Signal B'])\n",
    "ax.set_title('Standard Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we have more information about not only the means, but also their variability (Signal B is a much less variable signal, and thus more reliable).\n",
    "\n",
    "We can express variability in a timeseries as well. For example, rather than just calculating the Global Field Potential of our data, we can calculate the standard error across channels.\n",
    "\n",
    "We'll use data_av, which is the average activity for each channel (averaging across epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data_av.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the standard error across channels\n",
    "data_global_ste = nds.stats.standard_error(data_av ** 2)\n",
    "print(data_global_ste.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we still have a timeseries, a useful function is `fill_between`, which lets us fill a range of values in the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we can plot the mean +/- standard error for each channel...\n",
    "fig, ax = plt.subplots()\n",
    "ax.fill_between(epochs.times, data_global - data_global_ste,\n",
    "                data_global + data_global_ste)\n",
    "ax.set_title('Standard Error for GFP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> * What do you think it means if the standard error is particularly small with GFP?\n",
    "> * What does it mean if the standard error is really big?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

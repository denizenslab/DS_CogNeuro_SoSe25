{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Thus far, we've been calculating statistics of our neural signals. That is, we have transformed our signal (e.g., filtering or calculating a TFR), and come up with some number to summarize it (e.g., average activity across trials).\n",
    "\n",
    "However, neuroscience is about **linking the world to brain function**, and the best way to do this is to build a *model* that links the two. This is a more explicit way of defining how a change in the world results in a change in the brain.\n",
    "\n",
    "Today, we'll cover the basics of **modeling**. We'll start with correlation, move to univariate regression, and we'll finish with multivariate regression.\n",
    "\n",
    "![](https://imgs.xkcd.com/comics/linear_regression.png)\n",
    "\n",
    "\n",
    "## Goals for today\n",
    "* Understand correlation in context of simulated signals\n",
    "* Understand correlation in the context of electrophysiology\n",
    "* Relate correlation to univariate regression\n",
    "* Use multivariate regression to ask more complicated questions about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datascience as ds\n",
    "import neurods as nds\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Correlation via simulation\n",
    "> * In general, what does correlation reflect? If we say that two numbers are correlated, what do we mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To dive into this question, we'll create two variables from scratch and see what we can do with them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll begin by simulating a few signals, this will help with the intuition for what regression means\n",
    "# First, we'll create a random variable\n",
    "noise_amp = 1\n",
    "n_pts = 50\n",
    "a = 10 * np.random.random(n_pts)\n",
    "\n",
    "# Now, we'll define a \"weight\" that causes a second variable to respond to it\n",
    "weight = 2\n",
    "\n",
    "# Finally, we'll create some noise so that it's not a perfect mapping\n",
    "noise = noise_amp * np.random.randn(n_pts)\n",
    "\n",
    "# Then let's mix them together. In this case, b is explicitly created from the values in a\n",
    "b = weight * a + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the signals\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(a, label='a')\n",
    "ax.plot(b, label='b')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into modeling, we'll begin with *correlation*. Look at the two plots above, they seem to be varying in similar ways. When one goes up, the other goes up, when one goes down, the other goes down. How can we quantify this?\n",
    "\n",
    "> * What's the explicit relationship between these two signals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it is better to investigate correlations between two signals by making a **scatterplot**. We'll use one here to see how related the two signals are related with one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.scatter(a, b)\n",
    "ax.set_xlabel('Signal A')\n",
    "ax.set_ylabel('Signal B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our intuition from above seem to hold here as well. When values of the x-axis are large, values on the y-axis also tend to be large. We can quantify this with the correlation coefficient.\n",
    "\n",
    "![](http://www.stat.yale.edu/Courses/1997-98/101/cor.gif)\n",
    "\n",
    "There are many functions we can use to calculate the correlation coefficient. Below we'll use `np.corrcoef`.\n",
    "\n",
    "> *Note: The `corrcoef` function will actually return a \"correlation matrix\". In this case, every row of `a` is correlated with every row of `b`, and displayed as a matrix. Since our variables are vectors, the output will be a 2 by 2 matrix and the 1st element of the 2nd row will be the correlation coefficient.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr = np.corrcoef(a, b)\n",
    "print(corr)\n",
    "print('\\n\\n')\n",
    "\n",
    "# then take the 1st val of the 2nd row.\n",
    "print(corr[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * What would happen if we increased the noise parameter when constructing the signal above?\n",
    "\n",
    "# Correlation vs. Regression\n",
    "\n",
    "These two signals are nearly perfectly correlated, this is because above we've defined one signal to be a *linear function* of the other signal. In other words, like this:\n",
    "\n",
    "$signal_a = w * signal_b + noise$\n",
    "\n",
    "Having a high correlation basically tells us that the above formula is a \"good\" description of the relationship between these two variables. However it doesn't tell us anything about what `w` actually is.\n",
    "\n",
    "However, what if we wanted to know the *actual* value of `w` used to general signal B from signal A?\n",
    "\n",
    "To do this, we can use a related tool called **regression**. In regression, we can find any weight that defines the relationship between two signals. Instead of just asking \"is this linear equation a good relationship between the two signals?\", we can ask \"what is the linear weight that describes the relationship between these two signals?\".\n",
    "\n",
    "For example, we could change the weight value above, and the underlying correlation would always be 1. However, regression will find a different value each time.\n",
    "\n",
    "\n",
    "# Using regression on these simulated signals\n",
    "\n",
    "In regression, we explicitly model one signal as a linear function of the other signal. The output of regression is a *weight* (not a correlation) that tells us how we can predict values of one signal using the other.\n",
    "\n",
    "Here's the equation for a linear model:\n",
    "\n",
    "$$y = \\sum_{i=1}^{n}w_i x_i + \\epsilon$$\n",
    "\n",
    "This says: each output $y$ is predicted by weighting each feature $x$ with by corresponding weight $w$, and summing them together, then adding random noise $\\epsilon$.\n",
    "\n",
    "Regression is a technique for inferring what these weights are, given a dataset of inputs and outputs. Because we're using this explicit equation to model the relationship between these signals, linear regression is often called the simplest form of **modeling**.\n",
    "\n",
    "> * What's the main difference between regression and correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression in Python: scikit-learn\n",
    "There are many packages to do regression in python, but we'll focus on a Machine Learning package called `scikit-learn`.\n",
    "\n",
    "Scikit-learn is **the** machine learning tool in python. It's also probably the most popular machine learning library across all languages right now. You could take an entire class JUST on machine learning with scikit-learn, but here is a very quick primer:\n",
    "\n",
    "1. Scikit-learn contains \"estimator\" objects, that basically are a way of fitting different kinds of models to your data. You can create an estimator object by calling calling it (similar to a function): `myestimator = EstimatorObject()`.\n",
    "2. Once you have an estimator object, you can fit it to data. Generally speaking, this means that we need *input* and *output* data. These always have a shape (n_samples, n_features).\n",
    "3. We fit the estimator by using its `fit` method: `myestimator.fit(X, y)`.\n",
    "4. After fitting the model to data, we can inspect the model coefficients that have been fit (these will exist as attributes created after calling `fit`, and always end in an underscore e.g.,: `myestimator.coef_`.\n",
    "5. We can also use that model to predict new outputs given some inputs. We do this with the `predict` method, e.g.: `y_new = myestimator.predict(X_new)`.\n",
    "\n",
    "> * Scikit-Learn also has excellent [tutorials](http://scikit-learn.org/stable/tutorial/) that describe how to do machine learning with the library.\n",
    "> * Specifically, here's the section on [Linear Regression](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html#linear-regression)\n",
    "\n",
    "We'll take a look at how to use this with some actual data below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The least-squares solution in linear regression\n",
    "The simplest way to find the weight that relates two signals to one another is to solve the \"least squares\" equation for the data.\n",
    "\n",
    "What does \"least-squares\" mean in this context? It refers to the *error* of our model. Specifically, we can define the error of the model as the difference between our model predictions, and the \"actual\" values of the outputs. Since this can be either positive or negative, we square the difference so we can combine across datapoints.\n",
    "\n",
    "$$error=\\sum_{i=1}^n{(y - \\hat{y})^2}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\hat{y} = \\sum_{i=1}^{n}w_i x_i$$\n",
    "\n",
    "When we \"solve\" for the least squares solution, we mean \"determine the value `w` for each feature so that we minimize the above error.\n",
    "\n",
    "Let's do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest regression object solves the \"Least Squares\" problem.\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll simulate more random data so it's easier to change parameters\n",
    "# First, we'll create a random variable\n",
    "noise_amp = 1\n",
    "n_pts = 50\n",
    "a = 10 * np.random.random(n_pts)\n",
    "\n",
    "# Now, we'll define a \"weight\" that causes a second variable to respond to it\n",
    "weight = 2\n",
    "\n",
    "# Finally, we'll create some noise so that it's not a perfect mapping\n",
    "noise = noise_amp * np.random.randn(n_pts)\n",
    "\n",
    "# Then let's mix them together. In this case, b is explicitly created from the values in a\n",
    "b = weight * a + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# First, we'll add an extra dimension to both \"a\" and \"b\"\n",
    "# This ensures that they are shape (n_samples, n_features)\n",
    "a = a[:, np.newaxis]\n",
    "b = b[:, np.newaxis]\n",
    "\n",
    "# We'll create our regression model, and fit it to the data we created\n",
    "# We won't fit an intercept, though it is easy to do so\n",
    "reg = LinearRegression(fit_intercept=False)\n",
    "reg.fit(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, calling the `fit` method tells the model to find a set of coefficents (in this case just a single number) that, when combined with each value of \"a\", predicts a value of \"b\".\n",
    "\n",
    "Now that the model is fit, we can access it's `coef_` attribute, or use it to predict new values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This is the relationship that the model has found between a and b\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a **model** of signal B, using signal A, we can make predictions about new values. Below, we'll create a range of values to \"test\" our model, and see what the model outputs for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# To test this out, we'll create range of values that span the values in \"a\"\n",
    "# Let's see what the model predicts for each test value:\n",
    "test_vals = np.linspace(a.min(), a.max(), 200)\n",
    "b_preds = reg.coef_[0] * test_vals\n",
    "f, ax = plt.subplots()\n",
    "ax.scatter(a, b, color='k')\n",
    "ax.scatter(test_vals, b_preds, color='r')\n",
    "ax.set_xlabel('Signal A')\n",
    "ax.set_ylabel('Signal B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn also makes this easy by giving you a `predict` method of an estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We could have also just used the `predict` method of our estimator:\n",
    "b_preds = reg.predict(test_vals[:, np.newaxis])\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.scatter(a, b, color='k')\n",
    "ax.plot(test_vals, b_preds, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the error in this model by drawing a line from each \"actual\" datapoint (the black dots), to each \"predicted\" datapoint (the red line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# First plot the same data as above\n",
    "ax.scatter(a, b, color='k')\n",
    "ax.plot(test_vals, b_preds, color='r')\n",
    "\n",
    "# Now plot error lines\n",
    "for a_actual, b_actual in zip(a, b):\n",
    "    # Fine the nearest predicted a to this \"actual\" a \n",
    "    ix_a_pred = np.argmin(np.abs(a_actual - test_vals))\n",
    "    \n",
    "    # Then look up its value on the line\n",
    "    closest_predicted_b = b_preds[ix_a_pred]\n",
    "    \n",
    "    ax.vlines(a_actual, b_actual, closest_predicted_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the relationship between correlation and regression? Well, if we were to convert both the inputs and the outputs into **standard units** (AKA, so they had a mean == 0, and a variance == 1), then regression would give us the exact same answer as correlation.\n",
    "\n",
    "In scikit-learn, this is called **scaling** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Scale our variables\n",
    "a_scaled = (a - np.mean(a)) / np.std(a)\n",
    "b_scaled = (b - np.mean(b)) / np.std(b)\n",
    "\n",
    "# Alternatively we could do this with scikit-learn\n",
    "from sklearn.preprocessing import scale\n",
    "a_scaled = scale(a)\n",
    "b_scaled = scale(b)\n",
    "\n",
    "plt.hist(a, label='raw')\n",
    "plt.hist(a_scaled, label='scaled')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now re-run the regression\n",
    "reg.fit(a_scaled, b_scaled)\n",
    "coef_scaled = reg.coef_[0, 0]\n",
    "\n",
    "# Note that the coefficient has changed:\n",
    "print(coef_scaled)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Why isn't the coefficient exactly 1?\n",
    "> * What would happen if we were to increase the noise levels when simulating the signals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how using the regression model allowed us to *predict* a new output. This turns out to be really useful, and we'll cover it in future lectures. For now, we'll take these methods into our neural data...\n",
    "\n",
    "# Correlation and Regression in neural signals\n",
    "One of the most challenging parts of neuroscience is figuring out how much information we can infer about the link between world and brain activity. Something that greatly affects this is how *correlated* all of our neural signals are with one another. Let's take a look at the correlations between our neural signals.\n",
    "\n",
    "## The dataset\n",
    "We'll use the same ECoG dataset that you used as homework last week.\n",
    "\n",
    "The subject is listening to chords - some of them are consonant, some of them a dissonant. We are recording the brain activity from electrodes placed directly on the surface of the subject's brain. Moreover, these electrodes tend to be centered over auditory corte. We'd like to figure out if the brain processes consonant and dissonant chords differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# First we'll load the data. We'll load a downsampled version so that it's faster\n",
    "path_ecog = '../../data/ecog/chords_task/'\n",
    "data = mne.io.Raw(path_ecog + 'ecog_resamp-raw.fif', preload=True)\n",
    "\n",
    "# We'll load the x/y positions of the sensors so we can plot on the brain\n",
    "melec = ds.Table.read_table(path_ecog + 'meta_chans.csv')\n",
    "melec = melec.where(~np.isnan(melec['x']))  # Drop electrodes without a position\n",
    "lt = mne.channels.read_layout(path_ecog + 'brain.lout')\n",
    "im = plt.imread(path_ecog + 'brain.png')\n",
    "\n",
    "# We'll only take the first 3 minutes to save space\n",
    "data.crop(0, 60 * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear relationships between channels\n",
    "One of the first questions we ask with a new dataset is \"how related are channels to one another?\" In other words, are they correlated?\n",
    "\n",
    "We can use the correlation coefficient to answer this question. We'll calculate the linear relationship between each channel and every other channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# As always, we'll begin by looking at the raw data.\n",
    "# This time try to tell if channels are correlated with one another\n",
    "_ = data.plot(scalings='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember - correlations between two signals mean that when one signal is larger, the other also tends to be larger, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We'll pick two neighboring channels at random, and construct a correlation matrix between them:\n",
    "ch_a = data._data[10]\n",
    "ch_b = data._data[11]\n",
    "np.corrcoef(ch_a, ch_b)[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's compare that with a third channel that isn't right next to the other two:\n",
    "ch_c = data._data[60]\n",
    "np.corrcoef(ch_a, ch_c)[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Why do you think these two correlation values are different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can visualize where these electrodes are with respect to one another on the brain...\n",
    "activity = np.zeros(data._data.shape[0])\n",
    "activity[[10, 11, 16]] = 1\n",
    "nds.viz.plot_activity_on_brain(melec['x'], melec['y'], activity, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly look at the correlation between all of the channels, we can construct a \"correlation matrix\".\n",
    "\n",
    "This is a matrix where each channel is correlated with all the other channels.\n",
    "\n",
    "The output is a (channels x channels) matrix, that is symmetric about its diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cc_mat = np.corrcoef(data._data)\n",
    "print(cc_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll plot the correlation matrix for this grid. Each row is a channel, each column is a channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.imshow(cc_mat, vmin=0, vmax=.5, cmap=plt.cm.RdBu_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * What kind of structure do you see in this image?\n",
    "\n",
    "> * What do you think that it means?\n",
    "\n",
    "> * Do you think we get more information from correlated channels, or uncorrelated channels?\n",
    "\n",
    "> * Why do you think two channels are correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can pull out one row of the correlation matrix.\n",
    "# This is one channel's correlation with all other channels.\n",
    "# Since we have the 2d locations of channels, we can visualize this\n",
    "ix_seed = 25\n",
    "cc_elec = cc_mat[ix_seed]\n",
    "\n",
    "# This is a helper function to plot activity per electrode on the brain\n",
    "ax = nds.viz.plot_activity_on_brain(melec['x'], melec['y'], cc_elec, im, vmin=0, vmax=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using regression to ask more complex questions\n",
    "If we want to do something more complex than looking at pairwise relationships between signals (like the correlation matrix above) then we are going to need regression.\n",
    "\n",
    "Regression is powerful because of how flexible it is. The only thing we need to do is define input variables, and find a set of weights that mixes them together to predict an output variable. This means that we can use multiple input variables for a single output variable, something called **multiple regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We'll begin by loading event times for this dataset\n",
    "time = ds.Table.read_table(path_ecog + 'meta_time.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert our time onsets from seconds to time indices\n",
    "ix_onsets = time['start'] * data.info['sfreq']\n",
    "ix_onsets = np.round(ix_onsets).astype(int)\n",
    "ix_types = np.where(time['type'] == 'consonant', 1, 2)\n",
    "events = np.vstack([ix_onsets, np.ones_like(ix_onsets), ix_types]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# As before, we could create an Epochs object, and calculate the average response for each one:\n",
    "epochs = mne.Epochs(data, events, preload=True)\n",
    "av = epochs.average()\n",
    "_ = av.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding what contributes to Global Field Power with regression\n",
    "Recall that we calculated Global Field Power as a marker for how much activity there was overall among the electrodes. But how can we tell which electrodes are contributing the most to this global field power?\n",
    "\n",
    "We can accomplish this by investigating the weights that we fit with a regression. It will tell us how much relationship each signal has to the overall GFP, which in turn will tell us which channels are contributing to it the most.\n",
    "\n",
    "> * How do we calculate the Global Field Potential? What in general does it reflect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll calculate the global field power for these events\n",
    "squared_ep = epochs._data ** 2\n",
    "gfp_ep = squared_ep.mean(1)\n",
    "\n",
    "# Now horizontally stack the epoched GFP / data so we can fit regressions\n",
    "gfp = np.hstack(gfp_ep)\n",
    "squared_raw = np.hstack(squared_ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a set of weights that we can interpret, we must **scale** the input features first, so that they all have ~the same amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll scale the inputs.\n",
    "X = scale(squared_raw, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the input channel activity we've created to run a regression.\n",
    "\n",
    "We'll fit a linear model, where the values of each channel are used to predict the global field power of the entire grid.\n",
    "\n",
    "Don't forget, sklearn expects inputs of shape `(n_samples, n_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Finally, we can use regression to predict the GFP using channel data\n",
    "mod = LinearRegression(fit_intercept=False)\n",
    "mod.fit(X.T, gfp[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients of this model represent how much each channel \"contributes\" to the GFP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The coefficients of this model represent how much each channel predicts GFP\n",
    "coefs_raw = mod.coef_[0]\n",
    "coefs_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing the variables is easier\n",
    "tab = ds.Table().with_columns([('name', data.ch_names),\n",
    "                               ('coef', coefs_raw)])\n",
    "tab.plot(select='coef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll visualize this on the brain with a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nds.viz.plot_activity_on_brain(melec['x'], melec['y'], mod.coef_[0], im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * What can you conclude from the plot above? Which channels are driving the GFP?\n",
    "> * Why would a channel be a large contributor to Global Field Power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let's see if that matches with the ERPs for this subject\n",
    "fig = av.plot_topo(lt, fig_background=im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "* Do you see a perfect match between the two?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
